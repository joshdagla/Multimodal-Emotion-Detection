{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8286808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import librosa\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b37a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = 8          # record length\n",
    "AUDIO_FS = 16000              # audio sampling rate used by audio model\n",
    "FRAME_RATE = 10  \n",
    "FRAME_INTERVAL = 0.5             # how many frames/sec to process from webcam\n",
    "W_IMG = 0.6                   # image weight for fusion\n",
    "W_AUD = 0.4                   # audio weight for fusion\n",
    "SHOW_PREVIEW = True           # show webcam preview while recording\n",
    "PREVIEW_WINDOW_NAME = \"Recording - press 'q' to quit early\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5625d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_WEIGHTS = r\"C:/Users/joshd/Documents/btech_project_full/btech_project/facial_emotion_detection/facial_emotion_detection_9400/runs/train/facial_emotion_detection_model/weights/best.pt\"\n",
    "AUDIO_MODEL_PATH = r\"C:/Users/joshd/Documents/btech_project_full/btech_project/audio_emotion_detection/audio_emotion_CNN_model.h5\"\n",
    "LABEL_ENCODER_PATH = r\"C:/Users/joshd/Documents/btech_project_full/btech_project/audio_emotion_detection/emotion_audio_label_encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f55bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified class ordering we will use for fusion\n",
    "UNIFIED = [\"angry\", \"calm\", \"disgust\", \"fearful\", \"happy\", \"neutral\", \"sad\", \"surprised\"]\n",
    "\n",
    "# Map YOLO -> unified (update if you used different labels)\n",
    "YOLO_TO_UNIFIED = {\n",
    "    \"anger\": \"angry\",\n",
    "    \"content\": \"calm\",\n",
    "    \"disgust\": \"disgust\",\n",
    "    \"fear\": \"fearful\",\n",
    "    \"happy\": \"happy\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"sad\": \"sad\",\n",
    "    \"surprise\": \"surprised\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa97b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRESS_MAP = {\n",
    "    \"happy\": \"none\",\n",
    "    \"content\": \"none\",\n",
    "    \"neutral\": \"mild\",\n",
    "    \"calm\": \"mild\",\n",
    "    \"surprised\": \"mild\",\n",
    "    \"sad\": \"mediocre\",\n",
    "    \"fear\": \"high\",\n",
    "    \"fearful\": \"high\",\n",
    "    \"anger\": \"high\",\n",
    "    \"angry\": \"high\",\n",
    "    \"disgust\": \"high\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05aa6f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO classes: {0: 'anger', 1: 'content', 2: 'disgust', 3: 'fear', 4: 'happy', 5: 'neutral', 6: 'sad', 7: 'surprise'}\n",
      "Audio encoder classes: [np.str_('angry'), np.str_('calm'), np.str_('disgust'), np.str_('fearful'), np.str_('happy'), np.str_('neutral'), np.str_('sad'), np.str_('surprised')]\n",
      "Unified classes: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading models...\")\n",
    "yolo_model = YOLO(YOLO_WEIGHTS)\n",
    "audio_model = tf.keras.models.load_model(AUDIO_MODEL_PATH)\n",
    "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "audio_classes = list(label_encoder.classes_) \n",
    "print(\"YOLO classes:\", yolo_model.names)\n",
    "print(\"Audio encoder classes:\", audio_classes)\n",
    "print(\"Unified classes:\", UNIFIED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f08c1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_result_to_unified_probs(result):\n",
    "\n",
    "    try:\n",
    "        boxes = result.boxes\n",
    "    except Exception:\n",
    "        boxes = None\n",
    "\n",
    "    unified_probs = np.zeros(len(UNIFIED), dtype=float)\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        # no detection -> zeros\n",
    "        return unified_probs\n",
    "\n",
    "    try:\n",
    "        confs = boxes.conf.cpu().numpy()\n",
    "    except Exception:\n",
    "        confs = np.array(boxes.conf)\n",
    "\n",
    "    best_idx = int(np.argmax(confs))\n",
    "    # get class id\n",
    "    try:\n",
    "        cls_tensor = boxes.cls[best_idx]\n",
    "        cls_id = int(cls_tensor.item())\n",
    "    except Exception:\n",
    "        # fallback: direct indexing\n",
    "        cls_id = int(boxes.cls[best_idx])\n",
    "\n",
    "    names = yolo_model.names\n",
    "    if isinstance(names, dict):\n",
    "        yolo_label = names[cls_id]\n",
    "    else:\n",
    "        yolo_label = names[cls_id]\n",
    "\n",
    "    unified_label = YOLO_TO_UNIFIED.get(yolo_label, None)\n",
    "    if unified_label is None:\n",
    "        # unknown mapping -> return zeros\n",
    "        return unified_probs\n",
    "\n",
    "    try:\n",
    "        conf_val = float(boxes.conf[best_idx].item())\n",
    "    except Exception:\n",
    "        conf_val = float(confs[best_idx])\n",
    "\n",
    "    unified_probs[UNIFIED.index(unified_label)] = conf_val\n",
    "    # normalize if sum > 0\n",
    "    s = unified_probs.sum()\n",
    "    if s > 0:\n",
    "        unified_probs = unified_probs / s\n",
    "    return unified_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b854e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_frame_to_unified_probs(frame):\n",
    "\n",
    "    results = yolo_model(frame)       \n",
    "    res0 = results[0]\n",
    "    return yolo_result_to_unified_probs(res0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd5d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_file_to_unified_probs(audio_path):\n",
    "\n",
    "    y, sr = librosa.load(audio_path, sr=AUDIO_FS)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=120)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    # resize to (120,174)\n",
    "    mel_db_resized = cv2.resize(mel_db, (174, 120))\n",
    "    # standardize\n",
    "    mel_db_resized = (mel_db_resized - mel_db_resized.mean()) / (mel_db_resized.std() + 1e-6)\n",
    "    inp = mel_db_resized[..., np.newaxis].astype(np.float32)\n",
    "    inp = np.expand_dims(inp, axis=0)   # (1,120,174,1)\n",
    "\n",
    "    # model predict -> returns probabilities in audio_classes order\n",
    "    preds = audio_model.predict(inp, verbose=0)[0]   # shape (num_audio_classes,)\n",
    "    # convert into unified ordering\n",
    "    unified_probs = np.zeros(len(UNIFIED), dtype=float)\n",
    "    for i, cls in enumerate(audio_classes):\n",
    "        if cls in UNIFIED:\n",
    "            unified_idx = UNIFIED.index(cls)\n",
    "            unified_probs[unified_idx] = preds[i]\n",
    "    # normalize\n",
    "    s = unified_probs.sum()\n",
    "    if s > 0:\n",
    "        unified_probs = unified_probs / s\n",
    "    return unified_probs\n",
    "\n",
    "def probs_to_label_and_conf(probs):\n",
    "    idx = int(np.argmax(probs))\n",
    "    return UNIFIED[idx], float(probs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b895604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_record_and_fuse(duration=DURATION, frame_interval=FRAME_INTERVAL, w_img=W_IMG, w_aud=W_AUD):\n",
    "    # temporary files\n",
    "    tmp_dir = tempfile.mkdtemp(prefix=\"live_fusion_\")\n",
    "    audio_tmp = os.path.join(tmp_dir, \"live_audio.wav\")\n",
    "\n",
    "    # Start audio recording (non-blocking)\n",
    "    print(f\"[Live] Starting {duration}s audio recording (fs={AUDIO_FS})...\")\n",
    "    audio_rec = sd.rec(int(duration * AUDIO_FS), samplerate=AUDIO_FS, channels=1, dtype='float32')\n",
    "    time.sleep(0.2)  # small delay to ensure recording started\n",
    "\n",
    "    # Start capturing frames at intervals\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot open webcam (0).\")\n",
    "    print(\"[Live] Capturing frames from webcam...\")\n",
    "    frames = []\n",
    "    start = time.time()\n",
    "    next_capture = start\n",
    "    while True:\n",
    "        now = time.time()\n",
    "        if now >= next_capture:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"[Live] Warning: failed to read frame from webcam.\")\n",
    "            else:\n",
    "                # store frame (BGR)\n",
    "                frames.append(frame.copy())\n",
    "                # optional: show small preview (comment out if not desired)\n",
    "                # cv2.imshow(\"Live (press q to quit)\", frame)\n",
    "            next_capture += frame_interval\n",
    "        # break when duration exceeded\n",
    "        if now - start >= duration:\n",
    "            break\n",
    "        # small sleep to avoid busy loop\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # Stop camera and audio\n",
    "    cap.release()\n",
    "    # if showing preview: cv2.destroyAllWindows()\n",
    "    sd.wait()  # wait for audio to finish\n",
    "    # write audio to disk (convert float32 [-1,1] to int16)\n",
    "    audio_array = (audio_rec.squeeze() * 32767).astype('int16')\n",
    "    wav_write(audio_tmp, AUDIO_FS, audio_array)\n",
    "    print(f\"[Live] Saved audio to: {audio_tmp}\")\n",
    "    print(f\"[Live] Captured {len(frames)} frames.\")\n",
    "\n",
    "    # IMAGE PREDICTION: average unified probs across captured frames\n",
    "    if len(frames) == 0:\n",
    "        img_unified_avg = np.zeros(len(UNIFIED), dtype=float)\n",
    "    else:\n",
    "        probs_list = []\n",
    "        for f in frames:\n",
    "            p = image_frame_to_unified_probs(f)\n",
    "            probs_list.append(p)\n",
    "        img_unified_avg = np.mean(np.stack(probs_list, axis=0), axis=0)\n",
    "        # normalize\n",
    "        s = img_unified_avg.sum()\n",
    "        if s > 0:\n",
    "            img_unified_avg = img_unified_avg / s\n",
    "\n",
    "    # AUDIO PREDICTION\n",
    "    aud_unified = audio_file_to_unified_probs(audio_tmp)\n",
    "\n",
    "    # Convert probs -> labels + confidences\n",
    "    img_label, img_conf = probs_to_label_and_conf(img_unified_avg)\n",
    "    aud_label, aud_conf = probs_to_label_and_conf(aud_unified)\n",
    "\n",
    "    # Fusion\n",
    "    fused = (w_img * img_unified_avg) + (w_aud * aud_unified)\n",
    "    s = fused.sum()\n",
    "    if s > 0:\n",
    "        fused = fused / s\n",
    "    fused_label, fused_conf = probs_to_label_and_conf(fused)\n",
    "\n",
    "    # Print nicely\n",
    "    print(\"\\n========== LIVE FUSION RESULTS ==========\")\n",
    "    print(f\"Duration recorded : {duration:.1f}s\")\n",
    "    print(f\"Image Model → {img_label} (confidence={img_conf:.3f})\")\n",
    "    print(f\"Audio Model → {aud_label} (confidence={aud_conf:.3f})\")\n",
    "    print(f\"FUSED RESULT → {fused_label} (confidence={fused_conf:.3f})\")\n",
    "    print(\"Detailed probabilities (UNIFIED order):\")\n",
    "    for cls_name, p in zip(UNIFIED, fused):\n",
    "        print(f\"  {cls_name:10s} : {p:.3f}\")\n",
    "    print(\"=========================================\\n\")\n",
    "\n",
    "    # cleanup temp files\n",
    "    try:\n",
    "        os.remove(audio_tmp)\n",
    "        os.rmdir(tmp_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        'image_label': img_label,\n",
    "        'image_conf': img_conf,\n",
    "        'image_probs': img_unified_avg,\n",
    "        'audio_label': aud_label,\n",
    "        'audio_conf': aud_conf,\n",
    "        'audio_probs': aud_unified,\n",
    "        'fused_label': fused_label,\n",
    "        'fused_conf': fused_conf,\n",
    "        'fused_probs': fused\n",
    "    }\n",
    "\n",
    "def stress_level_from_label(label):\n",
    "    STRESS_ORDER = {\n",
    "        \"none\": 0,\n",
    "        \"mild\": 1,\n",
    "        \"mediocre\": 2,\n",
    "        \"high\": 3\n",
    "    }\n",
    "    return STRESS_MAP.get(label, \"mild\"), STRESS_ORDER[STRESS_MAP.get(label, \"mild\")]\n",
    "\n",
    "def assess_stress(img_label, audio_label):\n",
    "    stress_img, score_img = stress_level_from_label(img_label)\n",
    "    stress_audio, score_audio = stress_level_from_label(audio_label)\n",
    "\n",
    "    # Take the stronger stress estimation\n",
    "    if score_img > score_audio:\n",
    "        return stress_img\n",
    "    else:\n",
    "        return stress_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c732e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting live webcam + mic demo.\n",
      "[Live] Starting 8s audio recording (fs=16000)...\n",
      "[Live] Capturing frames from webcam...\n",
      "[Live] Saved audio to: C:\\Users\\Admin\\AppData\\Local\\Temp\\live_fusion_u16p26tg\\live_audio.wav\n",
      "[Live] Captured 17 frames.\n",
      "\n",
      "0: 480x640 1 anger, 1 sad, 126.5ms\n",
      "Speed: 10.9ms preprocess, 126.5ms inference, 144.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 1 sad, 56.1ms\n",
      "Speed: 1.6ms preprocess, 56.1ms inference, 13.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 1 sad, 68.7ms\n",
      "Speed: 1.0ms preprocess, 68.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 1 sad, 75.6ms\n",
      "Speed: 1.1ms preprocess, 75.6ms inference, 13.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 1 sad, 81.7ms\n",
      "Speed: 5.8ms preprocess, 81.7ms inference, 6.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 1 sad, 86.1ms\n",
      "Speed: 9.6ms preprocess, 86.1ms inference, 9.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 89.5ms\n",
      "Speed: 7.2ms preprocess, 89.5ms inference, 13.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 77.9ms\n",
      "Speed: 9.4ms preprocess, 77.9ms inference, 12.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 94.6ms\n",
      "Speed: 6.0ms preprocess, 94.6ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 38.3ms\n",
      "Speed: 1.5ms preprocess, 38.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 20.5ms\n",
      "Speed: 1.5ms preprocess, 20.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 41.7ms\n",
      "Speed: 1.4ms preprocess, 41.7ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 25.7ms\n",
      "Speed: 2.3ms preprocess, 25.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 45.8ms\n",
      "Speed: 1.4ms preprocess, 45.8ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 33.7ms\n",
      "Speed: 1.9ms preprocess, 33.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 40.6ms\n",
      "Speed: 1.3ms preprocess, 40.6ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 neutral, 15.9ms\n",
      "Speed: 1.6ms preprocess, 15.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "========== LIVE FUSION RESULTS ==========\n",
      "Duration recorded : 8.0s\n",
      "Image Model → neutral (confidence=0.765)\n",
      "Audio Model → fearful (confidence=0.871)\n",
      "FUSED RESULT → neutral (confidence=0.464)\n",
      "Detailed probabilities (UNIFIED order):\n",
      "  angry      : 0.001\n",
      "  calm       : 0.002\n",
      "  disgust    : 0.001\n",
      "  fearful    : 0.348\n",
      "  happy      : 0.005\n",
      "  neutral    : 0.464\n",
      "  sad        : 0.152\n",
      "  surprised  : 0.026\n",
      "=========================================\n",
      "\n",
      "Stress Level: High\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting live webcam + mic demo.\")\n",
    "    result = live_record_and_fuse(duration=DURATION)\n",
    "    stress = assess_stress(result['image_label'], result['audio_label'])\n",
    "    print(f\"Stress Level: {stress.capitalize()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
